{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpPHz2Lp6VD"
      },
      "source": [
        "# TP Coding autoencoders and variational autoencoders in Pytorch\n",
        "\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:\n",
        "\n",
        "\n",
        "![AUTOENCODER](https://perso.telecom-paristech.fr/anewson/doc/images/autoencoder_illustration_2.png)\n",
        "\n",
        "The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n",
        "\n",
        "### Your task:\n",
        "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE)\n",
        "\n",
        "\n",
        "First of all, let's load some packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JqNeIJ8Op8Ao"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def pytorch_to_numpy(x):\n",
        "  return x.detach().numpy()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyj5dj_eui9D"
      },
      "source": [
        "First, we load the mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4YPLKlPrufSk"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 128\n",
        "\n",
        "# MNIST Dataset\n",
        "mnist_trainset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_testset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "#create data loader with smaller dataset size\n",
        "max_mnist_size = 1000\n",
        "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0] \n",
        "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "\n",
        "# download test dataset\n",
        "max_mnist_size = 512\n",
        "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0] \n",
        "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset_reduced, batch_size=batch_size, shuffle=True,drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "r7YhlBT2PN9I"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mnist_trainset_reduced.dataset.data.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bkK4ktwfvC"
      },
      "source": [
        "# 1 Vanilla Autoencoder\n",
        "\n",
        "Now, we define the general parameters of the autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mD56EDzbvUxq"
      },
      "outputs": [],
      "source": [
        "# autoencoder parameters\n",
        "n_rows = mnist_trainset_reduced.dataset.data.shape[1]\n",
        "n_cols = mnist_trainset_reduced.dataset.data.shape[2]\n",
        "n_channels = 1\n",
        "n_pixels = n_rows*n_cols\n",
        "\n",
        "img_shape = (n_rows, n_cols, n_channels)\n",
        "n_epochs = 150"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLa2-jQwxSI"
      },
      "source": [
        "Now, define the autoencoder architecture. In the first part, we will use the following MLP architecture :\n",
        "\n",
        "Encoder :\n",
        "- Flatten input\n",
        "- Dense layer, output size h_dim_1 + ReLU\n",
        "- Dense layer, output size h_dim_2 + ReLU\n",
        "- Dense layer, output size z_dim (no non-linearity)\n",
        "\n",
        "Decoder :\n",
        "- Dense layer, output size h_dim_2 + ReLU\n",
        "- Dense layer, output size h_dim_1 + ReLU\n",
        "- Dense layer, output size x_dim + Sigmoid Activation\n",
        "- Reshape, to size $28\\times 28\\times 1$\n",
        "\n",
        "For the Reshape operation, use the ```A.view(dim_1,dim_2,...)``` function, where ```A``` is your tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape:  (28, 28, 1)\n",
            "Number of pixels:  784\n",
            "Number of rows:  28\n",
            "Number of columns:  28\n",
            "Number of channels:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Image shape: \", img_shape)\n",
        "print(\"Number of pixels: \", n_pixels)\n",
        "print(\"Number of rows: \", n_rows)\n",
        "print(\"Number of columns: \", n_cols)\n",
        "print(\"Number of channels: \", n_channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tEuZfnUlXxMl"
      },
      "outputs": [],
      "source": [
        "class AE(torch.nn.Module ):\n",
        "  def __init__(self, x_dim, h_dim1, h_dim2, z_dim,n_rows,n_cols,n_channels):\n",
        "    super(AE, self).__init__()\n",
        "\n",
        "    self.n_rows = n_rows\n",
        "    self.n_cols = n_cols\n",
        "    self.n_channels = n_channels\n",
        "    self.n_pixels = (self.n_rows)*(self.n_cols)\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    # encoder part\n",
        "    self.fc1 = nn.Linear (self.n_pixels, h_dim1) # FILL IN CODE HERE\n",
        "    self.fc2 = nn.Linear (h_dim1, h_dim2)# FILL IN CODE HERE\n",
        "    self.fc3 = nn.Linear (h_dim2, z_dim) # FILL IN CODE HERE \n",
        "    # decoder part\n",
        "    self.fc4 = nn.Linear (z_dim, h_dim2) # FILL IN CODE HERE\n",
        "    self.fc5 = nn.Linear (h_dim2, h_dim1) # FILL IN CODE HERE\n",
        "    self.fc6 = nn.Linear (h_dim1, x_dim) # FILL IN CODE HERE \n",
        "\n",
        "  def encoder(self, x):\n",
        "    h = x.view((-1, self.n_pixels)) # FILL IN CODE HERE\n",
        "    # h = torch.nn.Sequential(self.fc1(x),self.fc2(x),self.fc3(x)) # FILL IN CODE HERE\n",
        "    h = F.relu(self.fc1(h)) # FILL IN CODE HERE\n",
        "    h = F.relu(self.fc2(h)) # FILL IN CODE HERE\n",
        "    h = self.fc3(h) # FILL IN CODE HERE\n",
        "    return h # FILL IN CODE HERE\n",
        "  \n",
        "  def decoder(self, z):\n",
        "    \n",
        "    # h = torch.nn.Sequential(self.fc4(z),self.fc5(z),self.fc6(z)) # FILL IN CODE HERE\n",
        "    # h = h.view((self.)) \n",
        "    h = F.relu(self.fc4(z)) # FILL IN CODE HERE\n",
        "    h = F.relu(self.fc5(h)) # FILL IN CODE HERE\n",
        "    h = torch.sigmoid(self.fc6(h)) # FILL IN CODE HERE\n",
        "    \n",
        "    h = h.view((-1, self.n_channels, self.n_rows, self.n_cols)) # FILL IN CODE HERE\n",
        "    return h # FILL IN CODE HERE\n",
        "  def forward(self, x):\n",
        "    y = self.decoder(self.encoder(x))  # FILL IN CODE HERE\n",
        "    return(y)\n",
        "  def loss_function(self,x, y):\n",
        "    bce_loss = F.binary_cross_entropy(y.view(-1, self.n_pixels), x, reduction='none') # FILL IN CODE HERE\n",
        "    # FILL IN CODE HERE\n",
        "    return torch.mean(bce_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oV40vRMQRoG1"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "ae_dim_1 = 512\n",
        "ae_dim_2 = 256\n",
        "z_dim = 10\n",
        "ae_model = AE(x_dim=n_pixels, h_dim1= ae_dim_1, h_dim2=ae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
        "ae_optimizer = optim.Adam(ae_model.parameters())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-659aM36xvXX"
      },
      "source": [
        "Now, define a generic function to train the model for one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wfqX6Brlxjyi"
      },
      "outputs": [],
      "source": [
        "def train_ae(ae_model,data_train_loader,epoch):\n",
        "\ttrain_loss = 0\n",
        "\tfor batch_idx, (data, _) in enumerate(data_train_loader):\n",
        "\t\tae_optimizer.zero_grad()\n",
        "\t\t\n",
        "\t\ty = ae_model.forward(data)# FILL IN CODE HERE\n",
        "\t\tloss_ae = ae_model.loss_function(data.view(-1, n_pixels),y) # FILL IN CODE HERE\n",
        "  \n",
        "\t\tloss_ae.backward()\n",
        "\t\ttrain_loss += loss_ae.item()\n",
        "\t\tae_optimizer.step()\n",
        "\t\t\n",
        "\t\tif batch_idx % 100 == 0:\n",
        "\t\t\tprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "\t\t\t\tepoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
        "\t\t\t\t100. * batch_idx / len(data_train_loader), loss_ae.item() / len(data)))\n",
        "\tprint('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w3EbmswSzJdK"
      },
      "source": [
        "We define a function to carry out testing on the autoencoder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "11q_0PSibZk-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/1000 (0%)]\tLoss: 0.005416\n",
            "====> Epoch: 0 Average loss: 0.0043\n",
            "Train Epoch: 1 [0/1000 (0%)]\tLoss: 0.003011\n",
            "====> Epoch: 1 Average loss: 0.0024\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLoss: 0.002241\n",
            "====> Epoch: 2 Average loss: 0.0020\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLoss: 0.002142\n",
            "====> Epoch: 3 Average loss: 0.0019\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLoss: 0.002064\n",
            "====> Epoch: 4 Average loss: 0.0018\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLoss: 0.002111\n",
            "====> Epoch: 5 Average loss: 0.0018\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLoss: 0.001992\n",
            "====> Epoch: 6 Average loss: 0.0018\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLoss: 0.002012\n",
            "====> Epoch: 7 Average loss: 0.0018\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLoss: 0.002012\n",
            "====> Epoch: 8 Average loss: 0.0018\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLoss: 0.001987\n",
            "====> Epoch: 9 Average loss: 0.0018\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLoss: 0.001977\n",
            "====> Epoch: 10 Average loss: 0.0017\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLoss: 0.001981\n",
            "====> Epoch: 11 Average loss: 0.0017\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLoss: 0.001965\n",
            "====> Epoch: 12 Average loss: 0.0017\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLoss: 0.001864\n",
            "====> Epoch: 13 Average loss: 0.0017\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLoss: 0.001770\n",
            "====> Epoch: 14 Average loss: 0.0016\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLoss: 0.001775\n",
            "====> Epoch: 15 Average loss: 0.0016\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLoss: 0.001673\n",
            "====> Epoch: 16 Average loss: 0.0016\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLoss: 0.001687\n",
            "====> Epoch: 17 Average loss: 0.0015\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLoss: 0.001587\n",
            "====> Epoch: 18 Average loss: 0.0014\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLoss: 0.001596\n",
            "====> Epoch: 19 Average loss: 0.0014\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLoss: 0.001516\n",
            "====> Epoch: 20 Average loss: 0.0013\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLoss: 0.001493\n",
            "====> Epoch: 21 Average loss: 0.0013\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLoss: 0.001403\n",
            "====> Epoch: 22 Average loss: 0.0013\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLoss: 0.001388\n",
            "====> Epoch: 23 Average loss: 0.0012\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLoss: 0.001323\n",
            "====> Epoch: 24 Average loss: 0.0012\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLoss: 0.001307\n",
            "====> Epoch: 25 Average loss: 0.0012\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLoss: 0.001336\n",
            "====> Epoch: 26 Average loss: 0.0012\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLoss: 0.001248\n",
            "====> Epoch: 27 Average loss: 0.0011\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLoss: 0.001211\n",
            "====> Epoch: 28 Average loss: 0.0011\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLoss: 0.001179\n",
            "====> Epoch: 29 Average loss: 0.0011\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLoss: 0.001238\n",
            "====> Epoch: 30 Average loss: 0.0011\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLoss: 0.001182\n",
            "====> Epoch: 31 Average loss: 0.0011\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLoss: 0.001171\n",
            "====> Epoch: 32 Average loss: 0.0010\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLoss: 0.001126\n",
            "====> Epoch: 33 Average loss: 0.0010\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLoss: 0.001211\n",
            "====> Epoch: 34 Average loss: 0.0010\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLoss: 0.001131\n",
            "====> Epoch: 35 Average loss: 0.0010\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLoss: 0.001110\n",
            "====> Epoch: 36 Average loss: 0.0010\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLoss: 0.001121\n",
            "====> Epoch: 37 Average loss: 0.0010\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLoss: 0.001140\n",
            "====> Epoch: 38 Average loss: 0.0010\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLoss: 0.001121\n",
            "====> Epoch: 39 Average loss: 0.0010\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLoss: 0.001037\n",
            "====> Epoch: 40 Average loss: 0.0010\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLoss: 0.001090\n",
            "====> Epoch: 41 Average loss: 0.0010\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLoss: 0.001066\n",
            "====> Epoch: 42 Average loss: 0.0010\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLoss: 0.001035\n",
            "====> Epoch: 43 Average loss: 0.0009\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLoss: 0.001079\n",
            "====> Epoch: 44 Average loss: 0.0009\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLoss: 0.001043\n",
            "====> Epoch: 45 Average loss: 0.0009\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLoss: 0.001080\n",
            "====> Epoch: 46 Average loss: 0.0009\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLoss: 0.000990\n",
            "====> Epoch: 47 Average loss: 0.0009\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLoss: 0.001006\n",
            "====> Epoch: 48 Average loss: 0.0009\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLoss: 0.001001\n",
            "====> Epoch: 49 Average loss: 0.0009\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLoss: 0.000973\n",
            "====> Epoch: 50 Average loss: 0.0009\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLoss: 0.000999\n",
            "====> Epoch: 51 Average loss: 0.0009\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLoss: 0.001018\n",
            "====> Epoch: 52 Average loss: 0.0009\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLoss: 0.000942\n",
            "====> Epoch: 53 Average loss: 0.0009\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLoss: 0.000989\n",
            "====> Epoch: 54 Average loss: 0.0009\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLoss: 0.000938\n",
            "====> Epoch: 55 Average loss: 0.0008\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLoss: 0.000937\n",
            "====> Epoch: 56 Average loss: 0.0009\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLoss: 0.000904\n",
            "====> Epoch: 57 Average loss: 0.0008\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLoss: 0.000930\n",
            "====> Epoch: 58 Average loss: 0.0008\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLoss: 0.000908\n",
            "====> Epoch: 59 Average loss: 0.0008\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLoss: 0.000893\n",
            "====> Epoch: 60 Average loss: 0.0008\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLoss: 0.000910\n",
            "====> Epoch: 61 Average loss: 0.0008\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLoss: 0.000872\n",
            "====> Epoch: 62 Average loss: 0.0008\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLoss: 0.000879\n",
            "====> Epoch: 63 Average loss: 0.0008\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLoss: 0.000927\n",
            "====> Epoch: 64 Average loss: 0.0008\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLoss: 0.000890\n",
            "====> Epoch: 65 Average loss: 0.0008\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLoss: 0.000854\n",
            "====> Epoch: 66 Average loss: 0.0008\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLoss: 0.000867\n",
            "====> Epoch: 67 Average loss: 0.0008\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLoss: 0.000879\n",
            "====> Epoch: 68 Average loss: 0.0008\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLoss: 0.000894\n",
            "====> Epoch: 69 Average loss: 0.0008\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLoss: 0.000861\n",
            "====> Epoch: 70 Average loss: 0.0008\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLoss: 0.000891\n",
            "====> Epoch: 71 Average loss: 0.0008\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLoss: 0.000875\n",
            "====> Epoch: 72 Average loss: 0.0008\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLoss: 0.000860\n",
            "====> Epoch: 73 Average loss: 0.0008\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLoss: 0.000830\n",
            "====> Epoch: 74 Average loss: 0.0008\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLoss: 0.000886\n",
            "====> Epoch: 75 Average loss: 0.0008\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLoss: 0.000812\n",
            "====> Epoch: 76 Average loss: 0.0007\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLoss: 0.000802\n",
            "====> Epoch: 77 Average loss: 0.0007\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLoss: 0.000824\n",
            "====> Epoch: 78 Average loss: 0.0007\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLoss: 0.000796\n",
            "====> Epoch: 79 Average loss: 0.0007\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLoss: 0.000797\n",
            "====> Epoch: 80 Average loss: 0.0007\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLoss: 0.000820\n",
            "====> Epoch: 81 Average loss: 0.0007\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLoss: 0.000841\n",
            "====> Epoch: 82 Average loss: 0.0007\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLoss: 0.000807\n",
            "====> Epoch: 83 Average loss: 0.0007\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLoss: 0.000776\n",
            "====> Epoch: 84 Average loss: 0.0007\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLoss: 0.000773\n",
            "====> Epoch: 85 Average loss: 0.0007\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLoss: 0.000809\n",
            "====> Epoch: 86 Average loss: 0.0007\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLoss: 0.000740\n",
            "====> Epoch: 87 Average loss: 0.0007\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLoss: 0.000785\n",
            "====> Epoch: 88 Average loss: 0.0007\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLoss: 0.000793\n",
            "====> Epoch: 89 Average loss: 0.0007\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLoss: 0.000762\n",
            "====> Epoch: 90 Average loss: 0.0007\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLoss: 0.000784\n",
            "====> Epoch: 91 Average loss: 0.0007\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLoss: 0.000796\n",
            "====> Epoch: 92 Average loss: 0.0007\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLoss: 0.000792\n",
            "====> Epoch: 93 Average loss: 0.0007\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLoss: 0.000755\n",
            "====> Epoch: 94 Average loss: 0.0007\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLoss: 0.000757\n",
            "====> Epoch: 95 Average loss: 0.0007\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLoss: 0.000768\n",
            "====> Epoch: 96 Average loss: 0.0007\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLoss: 0.000761\n",
            "====> Epoch: 97 Average loss: 0.0007\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLoss: 0.000743\n",
            "====> Epoch: 98 Average loss: 0.0007\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLoss: 0.000770\n",
            "====> Epoch: 99 Average loss: 0.0007\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLoss: 0.000744\n",
            "====> Epoch: 100 Average loss: 0.0007\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLoss: 0.000758\n",
            "====> Epoch: 101 Average loss: 0.0007\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLoss: 0.000722\n",
            "====> Epoch: 102 Average loss: 0.0007\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLoss: 0.000725\n",
            "====> Epoch: 103 Average loss: 0.0007\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLoss: 0.000734\n",
            "====> Epoch: 104 Average loss: 0.0007\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLoss: 0.000733\n",
            "====> Epoch: 105 Average loss: 0.0007\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLoss: 0.000731\n",
            "====> Epoch: 106 Average loss: 0.0007\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLoss: 0.000714\n",
            "====> Epoch: 107 Average loss: 0.0007\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLoss: 0.000715\n",
            "====> Epoch: 108 Average loss: 0.0007\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLoss: 0.000749\n",
            "====> Epoch: 109 Average loss: 0.0006\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLoss: 0.000725\n",
            "====> Epoch: 110 Average loss: 0.0006\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLoss: 0.000708\n",
            "====> Epoch: 111 Average loss: 0.0006\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLoss: 0.000705\n",
            "====> Epoch: 112 Average loss: 0.0006\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLoss: 0.000734\n",
            "====> Epoch: 113 Average loss: 0.0006\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLoss: 0.000695\n",
            "====> Epoch: 114 Average loss: 0.0006\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLoss: 0.000696\n",
            "====> Epoch: 115 Average loss: 0.0006\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLoss: 0.000683\n",
            "====> Epoch: 116 Average loss: 0.0006\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLoss: 0.000686\n",
            "====> Epoch: 117 Average loss: 0.0006\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLoss: 0.000707\n",
            "====> Epoch: 118 Average loss: 0.0006\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLoss: 0.000686\n",
            "====> Epoch: 119 Average loss: 0.0006\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLoss: 0.000696\n",
            "====> Epoch: 120 Average loss: 0.0006\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLoss: 0.000680\n",
            "====> Epoch: 121 Average loss: 0.0006\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLoss: 0.000680\n",
            "====> Epoch: 122 Average loss: 0.0006\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLoss: 0.000704\n",
            "====> Epoch: 123 Average loss: 0.0006\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLoss: 0.000669\n",
            "====> Epoch: 124 Average loss: 0.0006\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLoss: 0.000677\n",
            "====> Epoch: 125 Average loss: 0.0006\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLoss: 0.000682\n",
            "====> Epoch: 126 Average loss: 0.0006\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLoss: 0.000691\n",
            "====> Epoch: 127 Average loss: 0.0006\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLoss: 0.000669\n",
            "====> Epoch: 128 Average loss: 0.0006\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLoss: 0.000691\n",
            "====> Epoch: 129 Average loss: 0.0006\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLoss: 0.000680\n",
            "====> Epoch: 130 Average loss: 0.0006\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLoss: 0.000693\n",
            "====> Epoch: 131 Average loss: 0.0006\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLoss: 0.000678\n",
            "====> Epoch: 132 Average loss: 0.0006\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLoss: 0.000680\n",
            "====> Epoch: 133 Average loss: 0.0006\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLoss: 0.000674\n",
            "====> Epoch: 134 Average loss: 0.0006\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLoss: 0.000681\n",
            "====> Epoch: 135 Average loss: 0.0006\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLoss: 0.000649\n",
            "====> Epoch: 136 Average loss: 0.0006\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLoss: 0.000627\n",
            "====> Epoch: 137 Average loss: 0.0006\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLoss: 0.000673\n",
            "====> Epoch: 138 Average loss: 0.0006\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLoss: 0.000663\n",
            "====> Epoch: 139 Average loss: 0.0006\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLoss: 0.000648\n",
            "====> Epoch: 140 Average loss: 0.0006\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLoss: 0.000634\n",
            "====> Epoch: 141 Average loss: 0.0006\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLoss: 0.000658\n",
            "====> Epoch: 142 Average loss: 0.0006\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLoss: 0.000641\n",
            "====> Epoch: 143 Average loss: 0.0006\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLoss: 0.000669\n",
            "====> Epoch: 144 Average loss: 0.0006\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLoss: 0.000662\n",
            "====> Epoch: 145 Average loss: 0.0006\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLoss: 0.000648\n",
            "====> Epoch: 146 Average loss: 0.0006\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLoss: 0.000637\n",
            "====> Epoch: 147 Average loss: 0.0006\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLoss: 0.000627\n",
            "====> Epoch: 148 Average loss: 0.0006\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLoss: 0.000646\n",
            "====> Epoch: 149 Average loss: 0.0006\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(0, n_epochs):\n",
        "  train_ae(ae_model,mnist_train_loader,epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "T8jXjdRyzMy2"
      },
      "outputs": [],
      "source": [
        "def display_images(imgs):\n",
        "  \n",
        "  r = 1\n",
        "  c = imgs.shape[0]\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    #black and white images\n",
        "    axs[j].imshow(pytorch_to_numpy(imgs[j, 0,:,:]), cmap='gray')\n",
        "    axs[j].axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def display_ae_images(ae_model, test_imgs):\n",
        "  n_images = 5\n",
        "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
        "  test_imgs = test_imgs[idx,:,:,:]\n",
        "  print(test_imgs.shape)\n",
        "\n",
        "  #get output images\n",
        "  output_imgs = pytorch_to_numpy(ae_model.forward( test_imgs ))\n",
        "  output_imgs = output_imgs.reshape((n_images, n_channels, n_rows, n_cols))\n",
        "  \n",
        "  r = 2\n",
        "  c = n_images\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  \n",
        "  for j in range(c):\n",
        "    #black and white images\n",
        "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[0,j].axis('off')\n",
        "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[1,j].axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9pbXch29d68D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 1, 28, 28])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(ae_model, test_imgs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "assPaJqB5sa-"
      },
      "source": [
        "__Question__ Are you satisfied with the results, do they look good ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7uoWa4O5x8R"
      },
      "source": [
        "__Answer:__ The results are quite satisfactory, as the images are clear and the digits are accurately reconstructed."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zO9ATQEiyr3b"
      },
      "source": [
        "## 2/ Two simple generative models\n",
        "\n",
        "In this section, we consider two naïve approaches to creating generative models. The general idea is the following:\n",
        "\n",
        "- train an autoencoder\n",
        "- estimate different statistics (average, variance) of the data in the latent space\n",
        "- using these statistics, define a model based on a Gaussian distribution\n",
        "- generate data with this distribution\n",
        "\n",
        "We will consider these two situations :\n",
        "\n",
        "- a multivariate Gaussian distribution with __diagonal covariance matrix__ (each latent component is an independent random variable). This requires the average and variance in each latent component\n",
        "- a multivariate Gaussian distribution with __non-diagonal covariance matrix__. This requires the average and covariance matrix of the latent components\n",
        "\n",
        "Obviously, since this is done _a posteriori_ after training the autoencoder, there is nothing which guarantees that the latent codes do indeed follow a Gaussian distribution. Our goal will be to verify that Variational Autoencoders indeed produce better results than such naïve approaches."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x2M1-BRmf56d"
      },
      "source": [
        "## 2.0 Defining and generating random Gaussian latent codes\n",
        "\n",
        "Let $z$ be a latent code and $d$ the dimension of the latent space (called ``z_dim`` in the code). We suppose that the $z$'s follow a multivariate Gaussian distribution, written as:\n",
        "\n",
        "\\begin{equation}\n",
        "z \\sim \\mathcal{N}\\left(\n",
        "\\mu,\n",
        "\\bf{C}\n",
        "\\right),\n",
        "\\end{equation}\n",
        "where $\\mu$ and $\\bf{C}$ are the average vector and covariance matrix of the Gaussian distribution. To define such a generative model, we must therefore determine $\\mu$ and $\\bf{C}$. Once this is done, we can generate a random Gaussian latent code in the following manner:\n",
        "\n",
        "\\begin{equation}\n",
        "z = \\mu + {\\bf{L}} \\varepsilon,\n",
        "\\end{equation}\n",
        "where $\\varepsilon$ is a random vector drawn from a multivariate normal distribution ($\\mu=0$ and ${\\bf{C}} = \\text{Id}$), and $\\bf{L}$ is the Cholesky decomposition of the positive semi-definite covariance matrix. In other words:\n",
        "\n",
        "\\begin{equation}\n",
        "{\\bf{C}} = {\\bf{L}}{\\bf{L}^T}.\n",
        "\\end{equation}\n",
        "\n",
        "This gives a simple method of producing a multivariate Gaussian random variable."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NWpucm972i7j"
      },
      "source": [
        "## 2.1/ A Gaussian model with diagonal covariance\n",
        "\n",
        "The first naïve model is  defined in this first case as:\n",
        "\n",
        "- $\\bf{\\mu}=\\left[\\mu_0, \\mu_1, \\cdots, \\mu_{d-1}\\right]^T$\n",
        "- $\n",
        "  \\bf{C} = \\begin{pmatrix}\n",
        "\\sigma_0^2 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_1^2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_{d-1}^2\n",
        "\\end{pmatrix}$\n",
        "\n",
        "In this situation, therefore, the matrix $\\bf{L}$ can be calculated quite simply, as:\n",
        "- $\n",
        "  \\bf{L} = \\begin{pmatrix}\n",
        "\\sigma_0 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_{d-1}\n",
        "\\end{pmatrix}$\n",
        "\n",
        "In the next cell, calculate the empirical average and variances over a certain number of batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sUXHCtvW2iQ0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average of latent codes: tensor([-7.9365, -6.8338,  2.0031,  0.3901,  5.6143,  6.1905,  0.8804, -1.2749,\n",
            "        -8.2555, -3.9020], grad_fn=<MeanBackward1>)\n",
            "Standard deviation of latent codes: tensor([ 7.7009,  8.8451,  9.0096,  6.5725,  7.5335, 10.0133,  7.6215,  9.8593,\n",
            "         9.3356, 10.9164], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ],
      "source": [
        "n_batches = np.floor( len(mnist_train_loader.dataset.indices)/batch_size ).astype(int)\n",
        "\n",
        "z_average = torch.zeros(n_batches,ae_model.z_dim)\n",
        "z_sigma = torch.zeros(n_batches,ae_model.z_dim)\n",
        "\n",
        "for batch_idx, (data, _) in enumerate(mnist_train_loader):\n",
        "  z = ae_model.encoder(data)\n",
        "  z_average[batch_idx,:] = torch.mean(z,dim=0) # FILL IN CODE HERE\n",
        "  z_sigma[batch_idx,:] = torch.std(z,dim=0) # FILL IN CODE HERE \n",
        "\n",
        "z_average = torch.mean(z_average, dim=0) # FILL IN CODE HERE\n",
        "z_sigma = torch.mean(z_sigma, dim=0) # FILL IN CODE HERE\n",
        "\n",
        "print(\"Average of latent codes:\",z_average)\n",
        "print(\"Standard deviation of latent codes:\",z_sigma)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrpc62ML9K4l"
      },
      "source": [
        "Now, in the next cell generate data with this simple generative model using the approach described above. Display these images with the ``display_images`` function. \n",
        "\n",
        "__NB__ You do not actually have to define the matrix $\\bf{L}$ in this case, an element-wise multiplication of two (properly chosen) vectors will suffice. To generate multivariate normal random variables you can use the following Pytorch function:\n",
        "\n",
        "- ```torch.randn```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "1_Tekii-9QEo"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images=5):\n",
        "    \n",
        "    epsilon = torch.randn(n_images, ae_model.z_dim)  # FILL IN CODE HERE\n",
        "    z_generated = z_average + z_sigma * epsilon  # FILL IN CODE HERE\n",
        "    imgs_generated = ae_model.decoder(z_generated)  # FILL IN CODE HERE\n",
        "    return imgs_generated\n",
        "\n",
        "imgs_generated = generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images=5)\n",
        "imgs_generated = imgs_generated.reshape((5, n_channels, n_rows, n_cols))\n",
        "display_images(imgs_generated)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xiNaEgLIloeA"
      },
      "source": [
        "As you should be able to see, these results are not that good. Let's try a slightly more sophisticated model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WjVPfkRKYMSh"
      },
      "source": [
        "## 2.1 Non-diagonal Gaussian model\n",
        "\n",
        "The second model uses a non-diagonal covariance matrix $\\bf{C}$ in the multivariate Gaussian distribution. In the next cell, calculate the average and covariance matrix over several batches of latent codes.\n",
        "\n",
        "__NB__ You can use the ```torch.cov``` function. Make sure to put the data in the right format for this (see documentation : https://pytorch.org/docs/stable/generated/torch.cov.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ArXgre39CD2H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average of latent codes: tensor([-7.9912, -6.9994,  2.3012,  0.3733,  5.6475,  6.5256,  0.9123, -1.1467,\n",
            "        -8.5851, -3.8351], grad_fn=<MeanBackward1>)\n",
            "Covariance matrix of latent codes: tensor([[ 60.2197,  14.5266, -23.3737,  11.8860,  -2.8902, -15.7523,  15.5243,\n",
            "          -6.2737, -14.5844,  11.2164],\n",
            "        [ 14.5266,  79.7533,   1.0681,  -6.4228,   7.1988, -42.5817, -14.2691,\n",
            "          -5.4425,  11.7323,  20.1277],\n",
            "        [-23.3737,   1.0681,  81.9588, -20.6834,  22.0913,   1.7433,  -1.8791,\n",
            "          30.3583, -12.5084, -15.4896],\n",
            "        [ 11.8860,  -6.4228, -20.6834,  43.6187,   4.1685,  -7.4793,   4.7596,\n",
            "          -2.7561,  -2.3109,   2.9699],\n",
            "        [ -2.8902,   7.1988,  22.0913,   4.1685,  56.4200, -32.6893,  18.7253,\n",
            "         -13.5922,  15.1658,   4.5007],\n",
            "        [-15.7523, -42.5817,   1.7433,  -7.4793, -32.6893, 100.2341,  -6.8660,\n",
            "           7.4285, -48.7773,   7.7404],\n",
            "        [ 15.5243, -14.2691,  -1.8791,   4.7596,  18.7253,  -6.8660,  58.3674,\n",
            "         -28.4950,   7.2062,  17.2927],\n",
            "        [ -6.2737,  -5.4425,  30.3583,  -2.7561, -13.5922,   7.4285, -28.4950,\n",
            "          95.4460, -46.0886, -35.8195],\n",
            "        [-14.5844,  11.7323, -12.5084,  -2.3109,  15.1658, -48.7773,   7.2062,\n",
            "         -46.0886,  87.2034, -16.2576],\n",
            "        [ 11.2164,  20.1277, -15.4896,   2.9699,   4.5007,   7.7404,  17.2927,\n",
            "         -35.8195, -16.2576, 119.7786]], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ],
      "source": [
        "n_batches = np.floor(len(mnist_train_loader.dataset.indices) / batch_size).astype(int)\n",
        "\n",
        "z_average = torch.zeros(n_batches, ae_model.z_dim)\n",
        "z_covariance = torch.zeros(n_batches, ae_model.z_dim, ae_model.z_dim)\n",
        "\n",
        "for batch_idx, (data, _) in enumerate(mnist_train_loader):\n",
        "    z = ae_model.encoder(data)\n",
        "    z_average[batch_idx, :] = torch.mean(z, dim=0)  # FILL IN CODE HERE\n",
        "    z_covariance[batch_idx, :, :] = torch.cov(z.T) # FILL IN CODE HERE\n",
        "\n",
        "z_average = torch.mean(z_average, dim=0)  # FILL IN CODE HERE\n",
        "z_covariance = torch.mean(z_covariance, dim=0)  # FILL IN CODE HERE\n",
        "\n",
        "print(\"Average of latent codes:\", z_average)\n",
        "print(\"Covariance matrix of latent codes:\", z_covariance)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JhXU8cnTZ0E8"
      },
      "source": [
        "Now, generate some samples with this distribution. In this case, you will actually have to calculate the Cholesky decomposition and find $\\bf{L}$. For this, you can use \n",
        "\n",
        "- ```torch.linalg.cholesky```\n",
        "\n",
        "In this model, you will need to carry out matrix multiplication over a batch of latent codes, which is a bit more complicated than the previous naïve model (which used element-wise vector multiplication). So you have two options:\n",
        "\n",
        "- copy the matrix $\\bf{L}$ several times and carry out batch matrix multiplication\n",
        "- simply loop and carry out normal matrix multiplication to produce each image (this has the disadvantage of not taking advantage of any parallelisation, but it should not matter too much).\n",
        "\n",
        "In the first case, you can use the following functions:\n",
        "\n",
        "- ```torch.bmm```\n",
        "- ```torch.tile```\n",
        "\n",
        "Fill in the function to generate images using this model now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "zXGlJTZ7Z4ed"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_images_non_diagonal_gaussian(ae_model, z_average, z_covariance, n_images=5):\n",
        "\n",
        "    # Calculate Cholesky decomposition of covariance matrix: C = L L^T\n",
        "    # Make sure that the first dimension is the batch dimension (with batch size n_images)\n",
        "    # You can use the torch.unsqueeze function for this\n",
        "    L = torch.linalg.cholesky(z_covariance)  # FILL IN CODE HERE\n",
        "    L = L.unsqueeze(0).repeat(n_images, 1, 1)  # FILL IN CODE HERE\n",
        "\n",
        "    epsilon = torch.randn(n_images, ae_model.z_dim)  # FILL IN CODE HERE\n",
        "    z_generated = z_average.unsqueeze(0).repeat(n_images, 1) + torch.bmm(L, epsilon.unsqueeze(2)).squeeze(2)  # FILL IN CODE HERE\n",
        "    \n",
        "    imgs_generated = ae_model.decoder(z_generated)  # FILL IN CODE HERE\n",
        "    return (imgs_generated)\n",
        "\n",
        "imgs_generated = generate_images_non_diagonal_gaussian(ae_model, z_average, z_covariance, n_images=5)\n",
        "imgs_generated = imgs_generated.reshape((5, n_channels, n_rows, n_cols))\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qLtsdri6zKEm"
      },
      "source": [
        "You should see some improvement, but we can do better than this. Thus, we turn to the variational autoencoder."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8UqeNhuSdnDt"
      },
      "source": [
        "# 3/ Variational autoencoder\n",
        "\n",
        "Now, we are going to create an variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder\n",
        "\n",
        "## Main idea\n",
        "\n",
        "The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools : \n",
        "\n",
        "- A specific architecture, where the encoder produces the average and variance of the latent codes\n",
        "- A specially designed loss function\n",
        "\n",
        "Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "The architecture of the VAE model is as follows:\n",
        "\n",
        "The encoder consists of:\n",
        "\n",
        "Encoder :\n",
        "- Flatten input\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer (no non-linarity) to produce the average, Dense layer (no non-linarity) to produce the variance (these last two layers are in parallel)\n",
        "\n",
        "Decoder :\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ Sigmoid Activation\n",
        "- Reshape, to size $28\\times 28\\times 1$\n",
        "\n",
        "\n",
        "## Variational Autoencoder loss\n",
        "\n",
        "Recall that for the VAE, the loss function is in fact a function to __maximise__. In fact, for implementation, you will see that it is easier to __minimise__ $-\\mathcal{L}$.\n",
        "\n",
        "In the case of an image which is represented by a set of __Bernoulli__ variables (which is relevant for mnist), the original loss function (to maximise) is written :\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L} &= \\log\\left(p_\\theta(x|z)\\right) - KL\\left( q_\\phi(z|x) \\; || \\; p_\\theta(z)\\right) \\\\\n",
        "    &= \\left(\\sum_{i} x_i \\log y_i + (1-x_i) \\log (1-y_i)\\right) - \\left(\\frac{1}{2} \\sum_j \\left( \\sigma_j^2 + \\mu_j^2 - \\log \\sigma_j^2 -1 \\right)\\right)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "where $i$ is summed over the image pixels, and $j$ is summed over the elements of the latent space. $\\sigma_j^2$ is the $j$th element of the latent space variance, and $\\mu_j$ is the $j$th element of the latent space mean.\n",
        "\n",
        "The left part of the loss (reconstruction error) can be implemented simply as the binary cross-entropy between the input x and the output y. Since we are __maximising__ $-$[binary cross-entropy] (look at the formula), this is equivalent to minimising the binary cross-entropy.\n",
        "\n",
        "For the right part of the equation (KL divergence), you need to implement it manually. \n",
        "\n",
        "The final loss is the average, over the batch size, of the sum of the reconstruction error (left part) and the KL divergence (right part). Be careful, in the formula, the sums over $i$ and $j$ are over the number of pixels and the number of latent elements, respectively. To achieve a sum rather than an average, you can use ```torch.nn.BCELoss(reduction='sum')()```, and the ```torch.sum()``` functions.\n",
        "\n",
        "As in the case of the normal autoencoder, you will need to flatten and then reshape the tensors at the beginning/end of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6siMHQLheM4T"
      },
      "outputs": [],
      "source": [
        "class VAE(torch.nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, n_rows, n_cols, n_channels):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.n_rows = n_rows\n",
        "        self.n_cols = n_cols\n",
        "        self.n_channels = n_channels\n",
        "        self.n_pixels = (self.n_rows) * (self.n_cols)\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # encoder part\n",
        "        self.fc1 = nn.Linear(x_dim, h_dim1)  # FILL IN CODE HERE\n",
        "        self.fc2 = nn.Linear(h_dim1, h_dim2)  # FILL IN CODE HERE\n",
        "        self.fc31 = nn.Linear(h_dim2, z_dim)  # FILL IN CODE HERE\n",
        "        self.fc32 = nn.Linear(h_dim2, z_dim)  # FILL IN CODE HERE\n",
        "        # decoder part\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim2)  # FILL IN CODE HERE\n",
        "        self.fc5 = nn.Linear(h_dim2, h_dim1)  # FILL IN CODE HERE\n",
        "        self.fc6 = nn.Linear(h_dim1, x_dim)  # FILL IN CODE HERE\n",
        "\n",
        "    def encoder(self, x):\n",
        "        #x = x.view(-1, n_channels * n_rows * n_cols)\n",
        "        h = F.relu(self.fc1(x))  # FILL IN CODE HERE\n",
        "        h = F.relu(self.fc2(h))  # FILL IN CODE HERE\n",
        "        return self.fc31(h), self.fc32(h)  # mu, log_var\n",
        "\n",
        "    def decoder(self, z):\n",
        "        h = F.relu(self.fc4(z))  # FILL IN CODE HERE\n",
        "        h = F.relu(self.fc5(h))  # FILL IN CODE HERE\n",
        "        h = torch.sigmoid(self.fc6(h)).view(-1, self.n_channels, self.n_rows, self.n_cols)  # FILL IN CODE HERE\n",
        "        return h # FILL IN CODE HERE\n",
        "\n",
        "    def sampling(self, mu, log_var):\n",
        "        # this function samples a Gaussian distribution, with average (mu) and standard deviation specified (using log_var)\n",
        "        std = torch.exp(log_var / 2)  # FILL IN CODE HERE\n",
        "        eps = torch.randn_like(std)  # FILL IN CODE HERE\n",
        "        return eps.mul(std).add_(mu)  # return z sample\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_mu, z_log_var = self.encoder(x.view(-1, n_channels * n_rows * n_cols))\n",
        "        z = self.sampling(z_mu, z_log_var)\n",
        "        return self.decoder(z), z_mu, z_log_var\n",
        "\n",
        "    def loss_function(self, x, y, mu, log_var):\n",
        "        reconstruction_error = torch.nn.BCELoss(reduction='sum')(y, x)  # FILL IN CODE HERE\n",
        "\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())  # FILL IN CODE HERE\n",
        "\n",
        "        return (reconstruction_error + KLD) / x.shape[0]  # FILL IN CODE HERE\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hk_9fDIphlsi"
      },
      "source": [
        "Now, create the model (similarly as above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "pVlpC2R3htyU"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "vae_dim_1 = ae_dim_1\n",
        "vae_dim_2 = ae_dim_2\n",
        "vae_model = VAE(x_dim=n_pixels, h_dim1= vae_dim_1, h_dim2=vae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
        "vae_optimizer = optim.Adam(vae_model.parameters())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NYKLF_oMh5HO"
      },
      "source": [
        "Finally, train the model. First modify the training function to the case of the vae."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "z6DjKTWmmssb"
      },
      "outputs": [],
      "source": [
        "def train_vae(vae_model, data_train_loader, epoch):\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(data_train_loader):\n",
        "        vae_optimizer.zero_grad()\n",
        "\n",
        "        # data = data.view(-1, n_channels * n_rows * n_cols)\n",
        "        y, z_mu, z_log_var = vae_model(data)  # FILL IN CODE HERE\n",
        "        loss_vae = vae_model.loss_function(data, y, z_mu, z_log_var)  # FILL IN CODE HERE\n",
        "        loss_vae.backward()\n",
        "        train_loss += loss_vae.item()\n",
        "        vae_optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
        "                100. * batch_idx / len(data_train_loader), loss_vae.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "L9JUUs6Kh8HB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/1000 (0%)]\tLoss: 4.256998\n",
            "====> Epoch: 0 Average loss: 3.2781\n",
            "Train Epoch: 1 [0/1000 (0%)]\tLoss: 2.457382\n",
            "====> Epoch: 1 Average loss: 1.8608\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLoss: 1.737534\n",
            "====> Epoch: 2 Average loss: 1.5480\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLoss: 1.651993\n",
            "====> Epoch: 3 Average loss: 1.4843\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLoss: 1.597711\n",
            "====> Epoch: 4 Average loss: 1.4725\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLoss: 1.660550\n",
            "====> Epoch: 5 Average loss: 1.4612\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLoss: 1.590022\n",
            "====> Epoch: 6 Average loss: 1.4401\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLoss: 1.599225\n",
            "====> Epoch: 7 Average loss: 1.4250\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLoss: 1.589097\n",
            "====> Epoch: 8 Average loss: 1.4227\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLoss: 1.557083\n",
            "====> Epoch: 9 Average loss: 1.4127\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLoss: 1.617246\n",
            "====> Epoch: 10 Average loss: 1.3985\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLoss: 1.597458\n",
            "====> Epoch: 11 Average loss: 1.3902\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLoss: 1.565199\n",
            "====> Epoch: 12 Average loss: 1.3742\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLoss: 1.564724\n",
            "====> Epoch: 13 Average loss: 1.3626\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLoss: 1.488895\n",
            "====> Epoch: 14 Average loss: 1.3545\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLoss: 1.480810\n",
            "====> Epoch: 15 Average loss: 1.3417\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLoss: 1.556401\n",
            "====> Epoch: 16 Average loss: 1.3363\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLoss: 1.480101\n",
            "====> Epoch: 17 Average loss: 1.3256\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLoss: 1.495270\n",
            "====> Epoch: 18 Average loss: 1.3106\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLoss: 1.465387\n",
            "====> Epoch: 19 Average loss: 1.2916\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLoss: 1.406053\n",
            "====> Epoch: 20 Average loss: 1.2794\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLoss: 1.442162\n",
            "====> Epoch: 21 Average loss: 1.2557\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLoss: 1.430360\n",
            "====> Epoch: 22 Average loss: 1.2501\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLoss: 1.365206\n",
            "====> Epoch: 23 Average loss: 1.2325\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLoss: 1.355937\n",
            "====> Epoch: 24 Average loss: 1.2228\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLoss: 1.293956\n",
            "====> Epoch: 25 Average loss: 1.2081\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLoss: 1.388238\n",
            "====> Epoch: 26 Average loss: 1.2065\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLoss: 1.388726\n",
            "====> Epoch: 27 Average loss: 1.2003\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLoss: 1.310796\n",
            "====> Epoch: 28 Average loss: 1.1920\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLoss: 1.335310\n",
            "====> Epoch: 29 Average loss: 1.1741\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLoss: 1.303747\n",
            "====> Epoch: 30 Average loss: 1.1695\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLoss: 1.300347\n",
            "====> Epoch: 31 Average loss: 1.1611\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLoss: 1.285511\n",
            "====> Epoch: 32 Average loss: 1.1493\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLoss: 1.234936\n",
            "====> Epoch: 33 Average loss: 1.1455\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLoss: 1.267172\n",
            "====> Epoch: 34 Average loss: 1.1366\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLoss: 1.267667\n",
            "====> Epoch: 35 Average loss: 1.1150\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLoss: 1.268268\n",
            "====> Epoch: 36 Average loss: 1.1001\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLoss: 1.218689\n",
            "====> Epoch: 37 Average loss: 1.0862\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLoss: 1.212543\n",
            "====> Epoch: 38 Average loss: 1.0720\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLoss: 1.214821\n",
            "====> Epoch: 39 Average loss: 1.0530\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLoss: 1.138520\n",
            "====> Epoch: 40 Average loss: 1.0463\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLoss: 1.147525\n",
            "====> Epoch: 41 Average loss: 1.0303\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLoss: 1.137552\n",
            "====> Epoch: 42 Average loss: 1.0223\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLoss: 1.167088\n",
            "====> Epoch: 43 Average loss: 1.0208\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLoss: 1.136359\n",
            "====> Epoch: 44 Average loss: 1.0051\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLoss: 1.111705\n",
            "====> Epoch: 45 Average loss: 0.9945\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLoss: 1.103784\n",
            "====> Epoch: 46 Average loss: 0.9934\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLoss: 1.127935\n",
            "====> Epoch: 47 Average loss: 0.9890\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLoss: 1.111572\n",
            "====> Epoch: 48 Average loss: 0.9774\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLoss: 1.099578\n",
            "====> Epoch: 49 Average loss: 0.9625\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLoss: 1.045028\n",
            "====> Epoch: 50 Average loss: 0.9649\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLoss: 1.018983\n",
            "====> Epoch: 51 Average loss: 0.9597\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLoss: 1.090374\n",
            "====> Epoch: 52 Average loss: 0.9591\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLoss: 1.049444\n",
            "====> Epoch: 53 Average loss: 0.9521\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLoss: 1.054453\n",
            "====> Epoch: 54 Average loss: 0.9521\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLoss: 1.087820\n",
            "====> Epoch: 55 Average loss: 0.9500\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLoss: 1.014486\n",
            "====> Epoch: 56 Average loss: 0.9445\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLoss: 1.033494\n",
            "====> Epoch: 57 Average loss: 0.9334\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLoss: 1.032569\n",
            "====> Epoch: 58 Average loss: 0.9298\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLoss: 1.020296\n",
            "====> Epoch: 59 Average loss: 0.9309\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLoss: 0.993486\n",
            "====> Epoch: 60 Average loss: 0.9219\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLoss: 1.013839\n",
            "====> Epoch: 61 Average loss: 0.9217\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLoss: 1.006508\n",
            "====> Epoch: 62 Average loss: 0.9097\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLoss: 1.039190\n",
            "====> Epoch: 63 Average loss: 0.9165\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLoss: 0.996190\n",
            "====> Epoch: 64 Average loss: 0.9101\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLoss: 0.983326\n",
            "====> Epoch: 65 Average loss: 0.9042\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLoss: 0.943166\n",
            "====> Epoch: 66 Average loss: 0.9011\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLoss: 0.990966\n",
            "====> Epoch: 67 Average loss: 0.8923\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLoss: 1.049464\n",
            "====> Epoch: 68 Average loss: 0.8927\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLoss: 1.001177\n",
            "====> Epoch: 69 Average loss: 0.8935\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLoss: 0.988899\n",
            "====> Epoch: 70 Average loss: 0.8927\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLoss: 0.986588\n",
            "====> Epoch: 71 Average loss: 0.8876\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLoss: 0.972244\n",
            "====> Epoch: 72 Average loss: 0.8806\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLoss: 0.992988\n",
            "====> Epoch: 73 Average loss: 0.8780\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLoss: 0.932087\n",
            "====> Epoch: 74 Average loss: 0.8710\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLoss: 0.966727\n",
            "====> Epoch: 75 Average loss: 0.8709\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLoss: 0.939406\n",
            "====> Epoch: 76 Average loss: 0.8645\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLoss: 0.978254\n",
            "====> Epoch: 77 Average loss: 0.8641\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLoss: 0.963618\n",
            "====> Epoch: 78 Average loss: 0.8653\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLoss: 0.997240\n",
            "====> Epoch: 79 Average loss: 0.8608\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLoss: 0.942478\n",
            "====> Epoch: 80 Average loss: 0.8606\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLoss: 0.954607\n",
            "====> Epoch: 81 Average loss: 0.8547\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLoss: 0.921295\n",
            "====> Epoch: 82 Average loss: 0.8471\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLoss: 0.917228\n",
            "====> Epoch: 83 Average loss: 0.8498\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLoss: 0.940883\n",
            "====> Epoch: 84 Average loss: 0.8458\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLoss: 0.942077\n",
            "====> Epoch: 85 Average loss: 0.8447\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLoss: 0.927587\n",
            "====> Epoch: 86 Average loss: 0.8409\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLoss: 0.936504\n",
            "====> Epoch: 87 Average loss: 0.8422\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLoss: 0.917395\n",
            "====> Epoch: 88 Average loss: 0.8346\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLoss: 0.949202\n",
            "====> Epoch: 89 Average loss: 0.8297\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLoss: 0.919559\n",
            "====> Epoch: 90 Average loss: 0.8303\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLoss: 0.947654\n",
            "====> Epoch: 91 Average loss: 0.8310\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLoss: 0.908671\n",
            "====> Epoch: 92 Average loss: 0.8270\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLoss: 0.943521\n",
            "====> Epoch: 93 Average loss: 0.8228\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLoss: 0.912316\n",
            "====> Epoch: 94 Average loss: 0.8212\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLoss: 0.932466\n",
            "====> Epoch: 95 Average loss: 0.8206\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLoss: 0.918727\n",
            "====> Epoch: 96 Average loss: 0.8161\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLoss: 0.870066\n",
            "====> Epoch: 97 Average loss: 0.8096\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLoss: 0.911816\n",
            "====> Epoch: 98 Average loss: 0.8119\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLoss: 0.933077\n",
            "====> Epoch: 99 Average loss: 0.8150\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLoss: 0.898838\n",
            "====> Epoch: 100 Average loss: 0.8052\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLoss: 0.879203\n",
            "====> Epoch: 101 Average loss: 0.8062\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLoss: 0.915343\n",
            "====> Epoch: 102 Average loss: 0.7977\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLoss: 0.891582\n",
            "====> Epoch: 103 Average loss: 0.8016\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLoss: 0.874500\n",
            "====> Epoch: 104 Average loss: 0.8022\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLoss: 0.917639\n",
            "====> Epoch: 105 Average loss: 0.7988\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLoss: 0.855023\n",
            "====> Epoch: 106 Average loss: 0.7993\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLoss: 0.867428\n",
            "====> Epoch: 107 Average loss: 0.7899\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLoss: 0.903965\n",
            "====> Epoch: 108 Average loss: 0.7886\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLoss: 0.870868\n",
            "====> Epoch: 109 Average loss: 0.7854\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLoss: 0.850161\n",
            "====> Epoch: 110 Average loss: 0.7865\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLoss: 0.862001\n",
            "====> Epoch: 111 Average loss: 0.7826\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLoss: 0.893718\n",
            "====> Epoch: 112 Average loss: 0.7867\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLoss: 0.848587\n",
            "====> Epoch: 113 Average loss: 0.7791\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLoss: 0.862528\n",
            "====> Epoch: 114 Average loss: 0.7815\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLoss: 0.879881\n",
            "====> Epoch: 115 Average loss: 0.7817\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLoss: 0.857187\n",
            "====> Epoch: 116 Average loss: 0.7736\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLoss: 0.848293\n",
            "====> Epoch: 117 Average loss: 0.7757\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLoss: 0.871782\n",
            "====> Epoch: 118 Average loss: 0.7721\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLoss: 0.855238\n",
            "====> Epoch: 119 Average loss: 0.7739\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLoss: 0.863514\n",
            "====> Epoch: 120 Average loss: 0.7734\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLoss: 0.858272\n",
            "====> Epoch: 121 Average loss: 0.7689\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLoss: 0.845033\n",
            "====> Epoch: 122 Average loss: 0.7662\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLoss: 0.848040\n",
            "====> Epoch: 123 Average loss: 0.7671\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLoss: 0.829367\n",
            "====> Epoch: 124 Average loss: 0.7633\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLoss: 0.870480\n",
            "====> Epoch: 125 Average loss: 0.7593\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLoss: 0.864449\n",
            "====> Epoch: 126 Average loss: 0.7584\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLoss: 0.856829\n",
            "====> Epoch: 127 Average loss: 0.7610\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLoss: 0.851002\n",
            "====> Epoch: 128 Average loss: 0.7571\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLoss: 0.834737\n",
            "====> Epoch: 129 Average loss: 0.7572\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLoss: 0.846067\n",
            "====> Epoch: 130 Average loss: 0.7549\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLoss: 0.810294\n",
            "====> Epoch: 131 Average loss: 0.7517\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLoss: 0.826925\n",
            "====> Epoch: 132 Average loss: 0.7450\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLoss: 0.811004\n",
            "====> Epoch: 133 Average loss: 0.7437\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLoss: 0.849095\n",
            "====> Epoch: 134 Average loss: 0.7505\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLoss: 0.820272\n",
            "====> Epoch: 135 Average loss: 0.7410\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLoss: 0.803346\n",
            "====> Epoch: 136 Average loss: 0.7414\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLoss: 0.802904\n",
            "====> Epoch: 137 Average loss: 0.7424\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLoss: 0.830794\n",
            "====> Epoch: 138 Average loss: 0.7428\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLoss: 0.839460\n",
            "====> Epoch: 139 Average loss: 0.7345\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLoss: 0.845331\n",
            "====> Epoch: 140 Average loss: 0.7367\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLoss: 0.804107\n",
            "====> Epoch: 141 Average loss: 0.7371\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLoss: 0.800250\n",
            "====> Epoch: 142 Average loss: 0.7334\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLoss: 0.811941\n",
            "====> Epoch: 143 Average loss: 0.7372\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLoss: 0.839135\n",
            "====> Epoch: 144 Average loss: 0.7337\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLoss: 0.802104\n",
            "====> Epoch: 145 Average loss: 0.7322\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLoss: 0.797339\n",
            "====> Epoch: 146 Average loss: 0.7279\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLoss: 0.810742\n",
            "====> Epoch: 147 Average loss: 0.7311\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLoss: 0.816612\n",
            "====> Epoch: 148 Average loss: 0.7282\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLoss: 0.819628\n",
            "====> Epoch: 149 Average loss: 0.7314\n"
          ]
        }
      ],
      "source": [
        "# now train the model\n",
        "# n_epochs = 150\n",
        "for epoch in range(0, n_epochs):\n",
        "  train_vae(vae_model,mnist_train_loader,epoch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uTfRje_AkKDr"
      },
      "source": [
        "Now, generate some images with the VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "41tXdNsFkKk5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_images_vae(vae_model,n_images = 5):\n",
        "\n",
        "  epsilon = torch.randn(n_images, vae_model.z_dim) # FILL IN CODE HERE\n",
        "  imgs_generated = vae_model.decoder(epsilon) # FILL IN CODE HERE\n",
        "  return(imgs_generated)\n",
        "\n",
        "imgs_generated = generate_images_vae(vae_model,n_images=5)\n",
        "imgs_generated = imgs_generated.reshape((5, n_channels, n_rows, n_cols))\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnvKoynzaFN"
      },
      "source": [
        "Do you think the results are better ? What difference can you see ? What advantage does the Variational Autoencoder have over the simple autoencoder model, even though the second autoencoder approach has a more complex probabilistic latent model (a full covariance matrix) ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uuYn3_PBzjkI"
      },
      "source": [
        "__Answer:__ Yes, the results are better. The images are clearer and the digits are more accurately reconstructed, realistic and diverse. However, the images are a bit blurry, as expected. AEs can sometimes generate images with slightly blurry details, as they try to optimize a balance between the reconstruction error and the regularization term in their loss function.\n",
        "\n",
        "The VAE has several advantages over simpler autoencoder models. VAEs regularize the latent space by enforcing a Gaussian distribution, which helps prevent overfitting and ensures a smooth latent space, facilitating easier sampling and generation of new data points. Additionally, it learns a structured latent space, allowing it to capture complex relationships and generate diverse samples. VAEs are also more robust to missing data, making them suitable for applications with incomplete data. Futhermore, VAEs offer advanced generative capabilities, enabling novel data point sampling for applications like image synthesis."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uXm-D9Ef9vYm"
      },
      "source": [
        "We will now compare the models quantitavely."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "04MddkzuE324"
      },
      "source": [
        "# 3 Evaluating and comparing the models\n",
        "\n",
        "We will evaluate the models, in the following manner:\n",
        "\n",
        "- we train a simple convolutional neural network classifier on mnist, to a good accuracy\n",
        "- we generate images with each model\n",
        "- we find the average of the highest probability of the images according to the classifier, for each model. If this value is high, it means that on average the classifier considers that the images look like a \n",
        "\n",
        "We will use the following convoluional architecture for the classifier:\n",
        "\n",
        "- conv2d, filter size  3×3 , 32 filters, stride=(2,2), padding=\"SAME\"\n",
        "- ReLU\n",
        "- conv2d, filter size  3×3 , 32 filters, stride=(2,2), padding=\"SAME\"\n",
        "- ReLU\n",
        "- MaxPool2D, stride=(2,2)\n",
        "- Flatten\n",
        "- Dense layer\n",
        "\n",
        "Now, define the model. To make things easier, use the ```torch.nn.Sequential``` API (there is no need for a Class in this simple case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "P87a-DkXFOCv"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 20\n",
        "batch_size = 64\n",
        "nb_classes = int(mnist_trainset.targets.max()+1)\n",
        "\n",
        "# number of convolutional filters to use\n",
        "nb_filters = 32\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "\n",
        "# --- Size of the successive layers\n",
        "n_h_0 = 1 #greyscale input images\n",
        "n_h_1 = nb_filters\n",
        "n_h_2 = nb_filters\n",
        "\n",
        "mnist_classification_model = nn.Sequential(\n",
        "    nn.Conv2d(n_h_0, n_h_1, kernel_size, stride=1, padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(n_h_1, n_h_2, kernel_size, stride=1, padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(pool_size, stride=(2, 2)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(14*14*n_h_2, nb_classes)\n",
        ")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # FILL IN CODE HERE\n",
        "optimizer = torch.optim.Adam(mnist_classification_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_sUZONxw2bnt"
      },
      "source": [
        "Create a function to calculate accuracy, instead of loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "aOww0ydr2fT0"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(x_pred,x_label):\n",
        "  acc = torch.sum(x_pred == x_label)/(x_pred.shape[0])\n",
        "  return acc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qw0vkZIqFcse"
      },
      "source": [
        "Now, train the model. You should be able to achieve an accuracy close to 1.00 within 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0FA8YoX2FcHP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:0 Train Loss:0.0120 Accuracy:0.6016\n",
            "Epoch:1 Train Loss:0.0053 Accuracy:0.7266\n",
            "Epoch:2 Train Loss:0.0027 Accuracy:0.8828\n",
            "Epoch:3 Train Loss:0.0022 Accuracy:0.9297\n",
            "Epoch:4 Train Loss:0.0015 Accuracy:0.9531\n",
            "Epoch:5 Train Loss:0.0007 Accuracy:0.9844\n",
            "Epoch:6 Train Loss:0.0009 Accuracy:0.9688\n",
            "Epoch:7 Train Loss:0.0002 Accuracy:0.9922\n",
            "Epoch:8 Train Loss:0.0001 Accuracy:1.0000\n",
            "Epoch:9 Train Loss:0.0001 Accuracy:1.0000\n",
            "Epoch:10 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:11 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:12 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:13 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:14 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:15 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:16 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:17 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:18 Train Loss:0.0000 Accuracy:1.0000\n",
            "Epoch:19 Train Loss:0.0000 Accuracy:1.0000\n"
          ]
        }
      ],
      "source": [
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "for epoch in range(0,n_epochs):\n",
        "  train_loss=0.0\n",
        "\n",
        "  for batch_idx, (imgs, labels) in enumerate(mnist_train_loader):\n",
        "\n",
        "    # set the gradients back to 0\n",
        "    optimizer.zero_grad()\n",
        "    predict=mnist_classification_model(imgs)\n",
        "    # apply loss function\n",
        "    loss=criterion(predict,labels)\n",
        "    acc = get_accuracy(torch.argmax(predict,dim=1),labels)\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss=loss.item()\n",
        "  print('Epoch:{} Train Loss:{:.4f} Accuracy:{:.4f}'.format(epoch,train_loss/imgs.shape[0],acc))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zFgN5LblFwTa"
      },
      "source": [
        "### Evaluate the average maximum prediction of the images generated by each generative model (higher is better)\n",
        "\n",
        "Now, we will evaluate the models. For each ones, produce a certain number of images, and put those images through the classification network. Then find the maximum class probability of each image, and average it over all the images. We will use this as a metric to evaluate each model.\n",
        "\n",
        "__CAREFUL__: the output of the network does __not__ include the Softmax layer, so you will have to carry it out, with:\n",
        "- ```torch.nn.Softmax()(...)```\n",
        "\n",
        "Define this metric now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "lCJ_0qqjOXHT"
      },
      "outputs": [],
      "source": [
        "def generative_model_score(imgs_in,classification_model):\n",
        "  gen_score = torch.mean(torch.max(F.softmax(classification_model(imgs_in), dim=1), dim=1).values) # FILL IN CODE HERE\n",
        "  return(gen_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yGq7YFg51UoP"
      },
      "source": [
        "Now, generate some images with each of the three models, and evaluate these models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "4-L4u2jhILFx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagonal gaussian generative model score :  0.8787621259689331\n",
            "Non diagonal gaussian generative model score :  0.9470975995063782\n",
            "Variational autoencoder model score:  0.9137097001075745\n"
          ]
        }
      ],
      "source": [
        "imgs_diagonal_gaussian = generate_images_diagonal_gaussian(ae_model,z_average,z_sigma,n_images = 50)\n",
        "imgs_non_diagonal_gaussian = generate_images_non_diagonal_gaussian(ae_model,z_average,z_covariance,n_images = 50)\n",
        "imgs_vae = generate_images_vae(vae_model,n_images=50)\n",
        "\n",
        "# average of maximum of first model \n",
        "diagonal_gaussian_score = float(generative_model_score(imgs_diagonal_gaussian,mnist_classification_model))\n",
        "non_diagonal_gaussian_score = float(generative_model_score(imgs_non_diagonal_gaussian,mnist_classification_model))\n",
        "vae_gaussian_score = float(generative_model_score(imgs_vae,mnist_classification_model))\n",
        "\n",
        "print(\"Diagonal gaussian generative model score : \",diagonal_gaussian_score)\n",
        "print(\"Non diagonal gaussian generative model score : \",non_diagonal_gaussian_score)\n",
        "print(\"Variational autoencoder model score: \",vae_gaussian_score) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sxvsG8FC1gNS"
      },
      "source": [
        "Please answer the following questions:\n",
        "\n",
        "- Which model is better quantitatively ? \n",
        "\n",
        "**Answer:** Quantitatively, the Non diagonal gaussian generative model performs the best among the three models. However, this result changes according to the set evaluated. \n",
        "\n",
        "- Do the quantitative result support the qualitative results ?\n",
        "\n",
        "**Answer:** No. The result of the diagonal gaussian generative model is clearly poorer than the other two models, but quantitatively the results do not differ that much. In addition, depending on the set evaluated the Diagonal and Non-diagonal gaussian models may present higher score than the VAE model, which is not consistent with the qualitative results.\n",
        "\n",
        "- Can you see any drawbacks of this method of evaluation ?\n",
        "\n",
        "**Answer:** The method consists in generate a set of images using a generative model (diagonal Gaussian model, diagonal Gaussian model or Variational Autoencoder); pass each generated image through a pre-trained classification model (as MNIST digits); for each image, find the maximum class probability predicted by the classification model (this value represents the confidence of the classification model in assigning the image to a specific class); calculate the average of these maximum class probabilities across all generated images. \n",
        "\n",
        "The resulting value represents the average maximum prediction of the images generated by a specific generative model. A higher value indicates that the generative model produces images that are more recognizable and confidently classified by the classification model. In other words, the images generated by the model are more likely to resemble the real data (in this case, the MNIST digits) and are less likely to be ambiguous or distorted.\n",
        "\n",
        "However, there are some problems with this method. The metric focuses on the maximum class probability, which means it could be sensitive to overconfident predictions by the classification model. If the classification model tends to assign high probabilities to specific classes, even when the generated images are not very realistic, the metric could be biased and overestimate the quality of the generative model.\n",
        "\n",
        "In addition, the metric only evaluates the recognizability of the generated images, but not their diversity. A generative model could produce a limited set of highly recognizable images, resulting in a high average maximum prediction score, but it might not capture the true diversity of the real data distribution. Also, it does not provide a direct measure of the visual quality, structure, or other important aspects of the generated images, since it indirectly measures the quality of the generated images through the classification model's predictions.\n",
        "\n",
        "\n",
        "- Can you propose any more sophisticated models than the multivariate Gaussian approach (apart from the variational autoencoder) ? \n",
        "\n",
        "**Answer:** There are several more sophisticated generative models beyond the multivariate Gaussian approach, such as Normalizing Flows. Normalizing Flows are a class of generative models that learn an invertible mapping between a simple distribution (e.g., Gaussian) and a more complex data distribution. They can model complex and high-dimensional distributions by composing multiple invertible transformations. This approach allows for efficient sampling, density estimation, and inference."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
